# **GenieRAG: Retrieval-Augmented Generation (RAG) with LLM**

## **ğŸ“Œ Overview**
GenieRAG is a **Retrieval-Augmented Generation (RAG) system** that combines **document retrieval** and **LLM-based response generation**. The system retrieves relevant document snippets and enhances an LLM's response quality by providing **contextualized answers**.

## **ğŸš€ Features**
âœ… Document retrieval using **FAISS** or **ChromaDB**
âœ… Embedding generation with **Hugging Face Sentence Transformers**
âœ… Response generation using **GPT-2**
âœ… FastAPI-powered REST API for interaction
âœ… NLP preprocessing and chunking using **LangChain**
âœ… Experiment tracking with **MLflow** (optional)

---

## **ğŸ› ï¸ Tech Stack**
| **Category**        | **Tool/Library Used** |
|---------------------|----------------------|
| **Language**       | Python 3.9+          |
| **LLM**            | GPT-2 (Hugging Face) |
| **Embeddings**     | Sentence Transformers (`all-MiniLM-L6-v2`) |
| **Vector Store**   | FAISS / ChromaDB     |
| **API Framework**  | FastAPI              |
| **Data Processing** | LangChain, pdfplumber, PyMuPDF |
| **Experiment Tracking** | MLflow (Optional) |

---

## **ğŸ“‚ Project Structure**
```
GenieRAG/
â”‚â”€â”€ models/                 # Store downloaded models
â”‚â”€â”€ data/                   # Input documents
â”‚â”€â”€ src/
â”‚   â”‚â”€â”€ vectorstore/
â”‚   â”‚   â”‚â”€â”€ embed.py         # Convert text into embeddings
â”‚   â”‚   â”‚â”€â”€ retrieve.py      # Retrieve relevant chunks from FAISS/ChromaDB
â”‚   â”‚â”€â”€ llm/
â”‚   â”‚   â”‚â”€â”€ generate_response.py  # Generate responses using GPT-2
â”‚   â”‚   â”‚â”€â”€ api.py           # FastAPI server
â”‚â”€â”€ requirements.txt         # Python dependencies
â”‚â”€â”€ README.md                # Project documentation
```

---

## **ğŸ“¦ Installation & Setup**
### **ğŸ”¹ Step 1: Clone the Repository**
```sh
git clone https://github.com/yourusername/GenieRAG.git
cd GenieRAG
```

### **ğŸ”¹ Step 2: Install Dependencies**
```sh
pip install -r requirements.txt
```

### **ğŸ”¹ Step 3: Download Pretrained Models**
Run the following command to download GPT-2 and embeddings model:
```sh
python -c "from transformers import AutoModelForCausalLM, AutoTokenizer; AutoModelForCausalLM.from_pretrained('gpt2'); AutoTokenizer.from_pretrained('gpt2')"
python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"
```

---

## **ğŸš€ Usage**
### **ğŸ”¹ Step 1: Create Document Embeddings**
```sh
python src/vectorstore/embed.py
```

### **ğŸ”¹ Step 2: Run the FastAPI Server**
```sh
python src/llm/api.py
```
Now, open **http://127.0.0.1:8000/docs** to interact with the API.

### **ğŸ”¹ Step 3: Query the API** (via Postman or cURL)
#### **Postman Instructions**
1ï¸âƒ£ Open **Postman** â†’ Create a new `POST` request.
2ï¸âƒ£ Set the URL to:
```
http://127.0.0.1:8000/ask
```
3ï¸âƒ£ Go to the **Body** tab â†’ Select `raw` â†’ Choose `JSON`
4ï¸âƒ£ Enter the following JSON payload:
```json
{
    "query": "What are the symptoms of plant disease?"
}
```
5ï¸âƒ£ Click `Send`

#### **Alternatively, use cURL:**
```sh
curl -X POST "http://127.0.0.1:8000/ask" -H "Content-Type: application/json" -d '{"query": "What are the causes of plant diseases?"}'
```

---

## **ğŸ“Œ Next Steps**
ğŸ”¹ Enhance the retriever by experimenting with **different embedding models**.
ğŸ”¹ Fine-tune the LLM for domain-specific tasks.
ğŸ”¹ Extend with **document ingestion automation** using Prefect or Airflow.

ğŸ’¡ **Enjoy building with RAG + NLP!** ğŸš€

